{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<h1>\n",
    "<center>CFRM 421/521, Spring 2022</center>\n",
    "</h1>\n",
    "\n",
    "<h1>\n",
    "<center>Daniel Eduardo Oliver</center>\n",
    "</h1>\n",
    "\n",
    "<h1>\n",
    "<center>Homework 2</center>\n",
    "</h1>\n",
    "\n",
    "* **Due: Monday, May 2, 2022, 11:59 PM**\n",
    "\n",
    "\n",
    "* Total marks: 41\n",
    "\n",
    "\n",
    "* Late submissions are allowed, but a 20% penalty per day applies. Your last submission is considered for calculating the penalty.\n",
    "\n",
    "\n",
    "*  Use this Jupyter notebook as a template for your solutions. **Your solution must be submitted as one Jupyter notebook on Canvas and one PDF file on Gradescope.** The notebook must be already run, that is, make sure that you have run all your code, save the notebook, and then when you reopen the notebook, checked that all output appears as expected. You are allowed to use code from the textbook, textbook website, or lecture notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Random forest for time series data [13 marks]\n",
    "\n",
    "In this question you will work with the NYSE dataset. Only 3 time series in this dataset will be use: `DJ_return` ($a_t$), `log_volatility` ($b_t$), and `log_volume` ($c_t$). Download the data as a csv file from [Canvas](https://canvas.uw.edu/files/91091313/download?download_frd=1). The data was originally obtained from the R library ISLR2, and you can read the documentation for the dataset [here](https://cran.rstudio.com/web/packages/ISLR2/ISLR2.pdf), which explains the meaning of the variables.\n",
    "\n",
    "You want to predict the 1-step ahead value of `log_volume` $c_{t+1}$ using the previous values of this variable and the other two variables (`DJ_return` and `log_volatility`) up to 5 lags. So the features are $c_{t},\\dots,c_{t-4},b_{t},\\dots,b_{t-4},a_{t},\\dots,a_{t-4}$.\n",
    "\n",
    "If the data is stored in a file named `NYSE.csv` in your working directory, then loading the data can be done using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.chdir('C:/Users/olive/OneDrive/Documents/AA UW CLASSES/A A A Spring 2022 Classes/CFRM 521/Assignments/HW2')\n",
    "data = pd.read_csv(\"NYSE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>DJ_return</th>\n",
       "      <th>log_volume</th>\n",
       "      <th>log_volatility</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962-12-03</td>\n",
       "      <td>mon</td>\n",
       "      <td>-0.004461</td>\n",
       "      <td>0.032573</td>\n",
       "      <td>-13.127403</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1962-12-04</td>\n",
       "      <td>tues</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>0.346202</td>\n",
       "      <td>-11.749305</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1962-12-05</td>\n",
       "      <td>wed</td>\n",
       "      <td>0.003845</td>\n",
       "      <td>0.525306</td>\n",
       "      <td>-11.665609</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1962-12-06</td>\n",
       "      <td>thur</td>\n",
       "      <td>-0.003462</td>\n",
       "      <td>0.210182</td>\n",
       "      <td>-11.626772</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1962-12-07</td>\n",
       "      <td>fri</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.044187</td>\n",
       "      <td>-11.728130</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date day_of_week  DJ_return  log_volume  log_volatility  train\n",
       "0  1962-12-03         mon  -0.004461    0.032573      -13.127403   True\n",
       "1  1962-12-04        tues   0.007813    0.346202      -11.749305   True\n",
       "2  1962-12-05         wed   0.003845    0.525306      -11.665609   True\n",
       "3  1962-12-06        thur  -0.003462    0.210182      -11.626772   True\n",
       "4  1962-12-07         fri   0.000568    0.044187      -11.728130   True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DJ_return</th>\n",
       "      <th>log_volume</th>\n",
       "      <th>log_volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.004461</td>\n",
       "      <td>0.032573</td>\n",
       "      <td>-13.127403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007813</td>\n",
       "      <td>0.346202</td>\n",
       "      <td>-11.749305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003845</td>\n",
       "      <td>0.525306</td>\n",
       "      <td>-11.665609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.003462</td>\n",
       "      <td>0.210182</td>\n",
       "      <td>-11.626772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.044187</td>\n",
       "      <td>-11.728130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DJ_return  log_volume  log_volatility\n",
       "0  -0.004461    0.032573      -13.127403\n",
       "1   0.007813    0.346202      -11.749305\n",
       "2   0.003845    0.525306      -11.665609\n",
       "3  -0.003462    0.210182      -11.626772\n",
       "4   0.000568    0.044187      -11.728130"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = data.drop(['date','day_of_week','train'], axis=1)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [3 marks]\n",
    "\n",
    "Create the feature matrix `X` and the target variable `y`. Print at least the first 2 rows of `X` and `y` (it is acceptable that not every element of the rows are printed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pandas import DataFrame\n",
    "# from pandas import concat\n",
    " \n",
    "# def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "# #     \n",
    "# #     Frame a time series as a supervised learning dataset.\n",
    "# #     Arguments:\n",
    "# #         data: Sequence of observations as a list or NumPy array.\n",
    "# #         n_in: Number of lag observations as input (X).\n",
    "# #         n_out: Number of observations as output (y).\n",
    "# #         dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "# #     Returns:\n",
    "# #         Pandas DataFrame of series framed for supervised learning.\n",
    "# #     \n",
    "#     n_vars = 1 if type(data) is list else data.shape[1]\n",
    "#     df = DataFrame(data)\n",
    "#     cols, names = list(), list()\n",
    "#     # input sequence (t-n, ... t-1)\n",
    "#     for i in range(n_in, 0, -1):\n",
    "#         cols.append(df.shift(i))\n",
    "#         names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "#     # forecast sequence (t, t+1, ... t+n)\n",
    "#     for i in range(0, n_out):\n",
    "#         cols.append(df.shift(-i))\n",
    "#         if i == 0:\n",
    "#             names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "#         else:\n",
    "#             names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "#     # put it all together\n",
    "#     agg = concat(cols, axis=1)\n",
    "#     agg.columns = names\n",
    "#     # drop rows with NaN values\n",
    "#     if dropnan:\n",
    "#         agg.dropna(inplace=True)\n",
    "#     return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = series_to_supervised(new_data.iloc[:,[0]],4)\n",
    "# # new_data.iloc[:,[1]]\n",
    "# d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# X = data[['DJ_return','log_volume','log_volatility']]\n",
    "\n",
    "# y = data[['log_volume']]\n",
    "# print(y.head(2))\n",
    "# print(X.head(2))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_split(ts, feature_steps=5, target_steps = 1):\n",
    "    n_obs = len(ts) - feature_steps - target_steps + 1\n",
    "    X = np.array([np.array(ts[idx:idx + feature_steps]).ravel('F') for idx in range(n_obs)])\n",
    "    y = np.array([ts.iloc[idx + feature_steps:idx + feature_steps + target_steps,1] #added .iloc[..:..,1] which grabs only the 2 column values\n",
    "                 for idx in range(n_obs)])\n",
    "    return X,y\n",
    "\n",
    "X,y = ts_split(new_data, feature_steps=5, target_steps=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = np.array(new_data[1:6])\n",
    "# print(n.ravel('F')[0:5])\n",
    "# print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X[0:,:,1]) #this grabs just the middle column of the of the first row of the data, confusing but data has 3 dimensions\n",
    "# print(X[0])\n",
    "# # X[[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data[\"log_volume\"].head(7) #checked for each variable and it looks like each time series column was successfully brought into X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two rows of X matrix (15 features per row):\n",
      "[[-4.46100000e-03  7.81300000e-03  3.84500000e-03 -3.46200000e-03\n",
      "   5.68000000e-04  3.25730000e-02  3.46202000e-01  5.25306000e-01\n",
      "   2.10182000e-01  4.41870000e-02 -1.31274026e+01 -1.17493047e+01\n",
      "  -1.16656090e+01 -1.16267724e+01 -1.17281302e+01]\n",
      " [ 7.81300000e-03  3.84500000e-03 -3.46200000e-03  5.68000000e-04\n",
      "  -1.08240000e-02  3.46202000e-01  5.25306000e-01  2.10182000e-01\n",
      "   4.41870000e-02  1.33246000e-01 -1.17493047e+01 -1.16656090e+01\n",
      "  -1.16267724e+01 -1.17281302e+01 -1.08725263e+01]]\n",
      "\n",
      "First two rows of y vector:\n",
      "[[ 0.133246]\n",
      " [-0.011528]]\n"
     ]
    }
   ],
   "source": [
    "y[1] #it works!!\n",
    "# new_data.iloc[0+5:0+5+1,1]\n",
    "\n",
    "print(\"First two rows of X matrix (15 features per row):\")\n",
    "print(X[0:2])\n",
    "print(\"\\nFirst two rows of y vector:\")\n",
    "print(y[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)  [4 marks]\n",
    "\n",
    "Consider fitting a random forest to predict the 1-step ahead value of `log_volume`. The random forest must include the argument `random_state=42`, and it is useful to also include `n_jobs=-1` (you can use `n_job=-1` throughout this homework wherever it is avaliable). Use 3-fold time series CV, with the test set split 50% into a validation set and 50% into the actual test set, to tune the hyperparameters `n_estimators` taking the values  100, 500, 750, and the cost-complexity pruning parameter $\\alpha$ taking the values $10^{-k}$, $k=0,1,\\dots,9$. The performance measure is RMSE. Report the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "series_len = new_data.size #is necessary for the plotting.\n",
    "\n",
    "#Hyperparameter combinations\n",
    "n_estimators_list = [100,500,750]\n",
    "ccp_alpha_list = [10**-0,10**-1,10**-2,10**-3,10**-4,10**-5,10**-6,10**-7,10**-8,10**-9] #cost-complexity pruning parameter alpha\n",
    "\n",
    "#code variation based on code in lecture 4 pg 99\n",
    "#we have to write our own code in order to do cross-validation and hyperparameter tuning on time series data\n",
    "def time_series_valid_test(X,y, n_split, valid_or_test,optimal_par=None):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "    rf_rmse = []\n",
    "    i = 0\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        i += 1\n",
    "        #break the test into 50% validation set and 50% test set\n",
    "        break_test_ind = int(test_index[0] + 0.5*(test_index[-1]-test_index[0]))\n",
    "        valid_index = np.array(list(range(test_index[0],break_test_ind)))\n",
    "        test_index = np.array(list(range(break_test_ind, test_index[-1])))\n",
    "    \n",
    "        #Split\n",
    "        X_train, X_valid, X_test = X[train_index], X[valid_index], X[test_index]\n",
    "        y_train, y_valid, y_test = y[train_index], y[valid_index], y[test_index]\n",
    "    \n",
    "        #tuning\n",
    "        if valid_or_test == \"valid\":\n",
    "            for ccp_alpha in ccp_alpha_list:\n",
    "                for n_estimators in n_estimators_list:\n",
    "                    model_rf = RandomForestRegressor(random_state=42, \n",
    "                                                     ccp_alpha = ccp_alpha,n_estimators=n_estimators,n_jobs=-1)\n",
    "                    model_rf.fit(X_train, y_train.ravel())#why do i need the .ravel() here?\n",
    "                    y_val_rf = model_rf.predict(X_valid)\n",
    "                    rf_rmse.append(np.sqrt(mean_squared_error(y_valid, y_val_rf)))\n",
    "                \n",
    "        # evaluate on test set\n",
    "        if valid_or_test == \"test\":\n",
    "            model_rf = RandomForestRegressor(random_state = 42, \n",
    "                                             ccp_alpha = 10**-5 ,n_estimators = 500,n_jobs=-1)#manually put in best hyperparameters\n",
    "            model_rf.fit(X_train, y_train.ravel())\n",
    "            y_test_rf = model_rf.predict(X_test)\n",
    "            rf_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_rf)))\n",
    "        \n",
    "            #plot the prediction for the last CV fold\n",
    "            if i == n_split:\n",
    "                plt.plot(range(series_len-test_index.size,series_len),\n",
    "                        y_test_rf, label=\"1-step ahead prediction\") #change this to one step ahead\n",
    "                plt.plot(range(series_len-test_index.size, series_len),\n",
    "                        y_test, label=\"True value\")\n",
    "                plt.legend(loc=\"upper left\")\n",
    "                \n",
    "    #average RMSE over CV folds\n",
    "    if valid_or_test == \"valid\":\n",
    "        rf_rmse = np.mean(np.array(rf_rmse).reshape(\n",
    "            n_split, len(ccp_alpha_list)*len(n_estimators_list)), axis=0) #will this axis=0 be a problem?\n",
    "        return rf_rmse\n",
    "    if valid_or_test == \"test\":\n",
    "        rf_rmse = np.mean(rf_rmse)\n",
    "        return rf_rmse, y_test_rf\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rmse = time_series_valid_test(X, y, 3, \"valid\") #30 parameter combinations, 3 CVs, take the average across the 3 CVs' parameter combos and out pos 30 babies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are n_estimators=500 and ccp_alpha=10^-5, with an average RMSE of (accross the 3 CV's):\n",
      "0.16042702494617542\n"
     ]
    }
   ],
   "source": [
    "#let's determine the best hyperparameters\n",
    "# print(np.argmin(rf_rmse))\n",
    "# print(rf_rmse.reshape(10,3))\n",
    "\n",
    "\n",
    "print(\"The best hyperparameters are n_estimators=500 and ccp_alpha=10^-5, with an average RMSE of (accross the 3 CV's):\")\n",
    "best_rmse = rf_rmse[16]\n",
    "print(best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)  [2 marks]\n",
    "\n",
    "Using the same time series split as in (b), compute the RMSE of the best fitting model on the test set, and include a plot of the true values and predicted values on the test set of the last fold (the fold closest to the current time) of the CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_rmse_test, y_test_rf_pred = time_series_valid_test(X, y, 3, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE Value of the best fitting model is:\n",
      "0.18697418303225452\n"
     ]
    }
   ],
   "source": [
    "print(\"The RMSE Value of the best fitting model is:\")\n",
    "print(rf_rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) [2 marks]\n",
    "\n",
    "It is often useful to check that your model is not worse than a very simple method of prediction. Compute the RMSE of a model that simply predicts the 1-step ahead value of `log_volume` $c_{t+1}$ as the current value $c_t$, and compare this to the best fitting random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_test_only(X,y, n_split, valid_or_test,optimal_par=None):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "    rmse = []\n",
    "    i = 0\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        i += 1\n",
    "        #break the test into 50% validation set and 50% test set\n",
    "        break_test_ind = int(test_index[0] + 0.5*(test_index[-1]-test_index[0]))\n",
    "        valid_index = np.array(list(range(test_index[0],break_test_ind)))\n",
    "        test_index = np.array(list(range(break_test_ind, test_index[-1])))\n",
    "    \n",
    "        #Split\n",
    "        X_train, X_valid, X_test = X[train_index], X[valid_index], X[test_index]\n",
    "        y_train, y_valid, y_test = y[train_index], y[valid_index], y[test_index]\n",
    "    \n",
    "        #tuning, HEre WE ARE ONLY WOrRIED ABOUt the tESt Set not thE VALIDAtION Set\n",
    "#         if valid_or_test == \"valid\":\n",
    "#             for ccp_alpha in ccp_alpha_list:\n",
    "#                 for n_estimators in n_estimators_list:\n",
    "#                     model_rf = RandomForestRegressor(random_state=42, \n",
    "#                                                      ccp_alpha = ccp_alpha,n_estimators=n_estimators,n_jobs=-1)\n",
    "#                     model_rf.fit(X_train, y_train.ravel())#why do i need the .ravel() here?\n",
    "#                     y_val_rf = model_rf.predict(X_valid)\n",
    "#                     rf_rmse.append(np.sqrt(mean_squared_error(y_valid, y_val_rf)))\n",
    "                \n",
    "        # evaluate on test set\n",
    "        if valid_or_test == \"test\":\n",
    "#             model_rf = RandomForestRegressor(random_state = 42, \n",
    "#                                              ccp_alpha = 10**-5 ,n_estimators = 500,n_jobs=-1)#manually put in best hyperparameters\n",
    "#             model_rf.fit(X_train, y_train.ravel())\n",
    "            y_test_pred = X_test #X_test here is exclusively the lagged value\n",
    "            rmse.append(np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "        \n",
    "#             #plot the prediction for the last CV fold\n",
    "#             if i == n_split:\n",
    "#                 plt.plot(range(series_len-test_index.size,series_len),\n",
    "#                         y_test_rf, label=\"1-step ahead prediction\") #change this to one step ahead\n",
    "#                 plt.plot(range(series_len-test_index.size, series_len),\n",
    "#                         y_test, label=\"True value\")\n",
    "#                 plt.legend(loc=\"upper left\")\n",
    "                \n",
    "    #average RMSE over CV folds\n",
    "#     if valid_or_test == \"valid\":\n",
    "#         rf_rmse = np.mean(np.array(rf_rmse).reshape(\n",
    "#             n_split, len(ccp_alpha_list)*len(n_estimators_list)), axis=0) #will this axis=0 be a problem?\n",
    "#         return rf_rmse\n",
    "    if valid_or_test == \"test\":\n",
    "        rmse = np.mean(rmse) #takes the average over the three folds\n",
    "        return rmse, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #talked about on pg 505 of the book and borrowed some code from pg 88 of the lecture 4 notes\n",
    "\n",
    "# split_ind = int(X.shape[0]*0.8)\n",
    "\n",
    "# X_train_full, y_train_full = X[:split_ind], y[:split_ind]\n",
    "# X_test, y_test = X[split_ind:], y[split_ind:]\n",
    "\n",
    "# y_pred = X_train_full[:,5]\n",
    "previous_lag = X[:,9]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "dumb_rmse, dumb_pred = time_series_test_only(previous_lag, y, 3, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the resulting RMSE of naive forecasting:\n",
      "0.21692013508591304\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is the resulting RMSE of naive forecasting:\")\n",
    "print(dumb_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) [2 marks]\n",
    "\n",
    "Compute the feature importances of the best fitting model. Which feature is the most important and what is its feature importance value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(ccp_alpha=1e-05, n_estimators=500, n_jobs=-1,\n",
       "                      random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_rf = RandomForestRegressor(random_state = 42,ccp_alpha = 10**-5 ,n_estimators = 500,n_jobs=-1)#manually put in best hyperparameters\n",
    "\n",
    "best_model_rf.fit(X, y.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important feature is the log_volatility value c_t (right before c_t+1). The feature importance is:\n",
      "0.5283695245558075\n",
      "All the weights are:\n",
      "[0.03375206 0.02975017 0.03719086 0.03905093 0.06408042 0.05909269\n",
      " 0.06268711 0.04354723 0.03707131 0.52836952 0.01552671 0.0122866\n",
      " 0.01101817 0.01258909 0.01398713]\n"
     ]
    }
   ],
   "source": [
    "# print(np.argmax(best_model_rf.feature_importances_))\n",
    "# print(best_model_rf.feature_importances_)\n",
    "print(\"The most important feature is the log_volatility value c_t (right before c_t+1). The feature importance is:\")\n",
    "print(best_model_rf.feature_importances_[9])\n",
    "\n",
    "print(\"All the weights are:\")\n",
    "print(best_model_rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SVM classification and regression [11 marks]\n",
    "\n",
    "## (a) [2 marks]\n",
    "\n",
    "In this question, a SVM is used for classification for the MNIST dataset. The following code loads the MNIST dataset, creates the test set, and to reduce training time, takes a random sample of 2000 points from the full training set to use as your actual training set stored in `X` and `y`. Do not shuffle the data and do not use a standard scaler.\n",
    "\n",
    "Hint: Reading the solution to Question 9 in the Chapter 5 [Jupyter notebook](https://github.com/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb) on the textbook website may help with this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, cache=True)\n",
    "mnist.target = mnist.target.astype(np.int8)\n",
    "X_train = mnist[\"data\"][:60000]\n",
    "X_test  = mnist[\"data\"][60000:]\n",
    "y_train = mnist[\"target\"][:60000]\n",
    "y_test  = mnist[\"target\"][60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "N = 2000\n",
    "split_obj = StratifiedShuffleSplit(n_splits=1,\n",
    "                               test_size=N/60000, random_state=42)\n",
    "for other_idx, subsample_idx in split_obj.split(X_train, y_train):\n",
    "    X = X_train[subsample_idx]\n",
    "    y = y_train[subsample_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Consider fitting the linear SVM classifier (`LinearSVC`) with `max_iter=50000`. For this model, optimize the hyperparameter $C$ using 3-fold CV over the values $10^{-k}$, $k=0,1,\\dots,9$, where the performance measure is accuracy. What is the best $C$ and what is the accuracy in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(max_iter=50000)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "linear_SVC = SVC(max_iter=50000)\n",
    "linear_SVC.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=SVC(max_iter=50000), n_jobs=-1,\n",
       "             param_grid={'C': [1, 0.1, 0.01, 0.001, 0.0001, 1e-05, 1e-06, 1e-07,\n",
       "                               1e-08, 1e-09]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here i used randomized search cross-validation\n",
    "from sklearn.model_selection import RandomizedSearchCV #code based off of code provided in Q9 of chapter 5\n",
    "from scipy.stats import reciprocal, uniform\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_distributions = {\"C\": [10**-0,10**-1,10**-2,10**-3,10**-4,10**-5,10**-6,10**-7,10**-8,10**-9]}\n",
    "grid_search_cv = GridSearchCV(linear_SVC, param_distributions, cv=3, n_jobs=-1, scoring=\"accuracy\")\n",
    "grid_search_cv.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best C hyperparameter is:\n",
      "SVC(C=1, max_iter=50000)\n",
      "\n",
      "With an accuracy score of:\n",
      "0.9855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = grid_search_cv.best_estimator_.predict(X)\n",
    "\n",
    "\n",
    "print(\"The best C hyperparameter is:\")\n",
    "print(grid_search_cv.best_estimator_)\n",
    "\n",
    "print(\"\\nWith an accuracy score of:\")\n",
    "print(accuracy_score(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [2 marks]\n",
    "\n",
    "**Task:** Now consider fitting a SVM with a Gaussian RBF kernel and `max_iter=50000`. For this model, optimize the hyperparameters $C$ over the distrbution `uniform(1,10)` and $\\gamma$ over the distribution `reciprocal(0.001, 0.1)` with 10 random samples. Again, use 3-fold CV and the performance measure is accuracy. What are the best hyperparameters and what is the accuracy in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import scipy.stats as sc\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "hyperparams = []\n",
    "avg_score = []\n",
    "for idx in range(10): #fitting 3 folds for each of 10 random samples! So there are 10 pairs of parameters\n",
    "    #draws from these distributions\n",
    "    gamma = sc.reciprocal.rvs(0.001,0.1)\n",
    "    C = uniform.rvs(1,10)\n",
    "    hyperparams.append([gamma, C])\n",
    "    \n",
    "\n",
    "    kernel_SVC = Pipeline([\n",
    "        (\"svc\", SVC(kernel = 'rbf', max_iter=50000, gamma=gamma, C=C))\n",
    "    ])\n",
    "\n",
    "    scores = cross_val_score(kernel_SVC, X, y, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "    avg_score.append(np.mean(scores))\n",
    "\n",
    "\n",
    "#     param_distributions = {\"gamma\": reciprocal(0.001,0.1), \"C\": uniform(1,10)}\n",
    "#     rnd_search_cv = RandomizedSearchCV(kernel_SVC, param_distributions, n_iter=10, verbose=2, cv=3, n_jobs=-1)\n",
    "#     rnd_search_cv.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11250005627816723, 0.11250005627816723, 0.11250005627816723, 0.11250005627816723, 0.11250005627816723, 0.11250005627816723, 0.11250005627816723, 0.11250005627816723, 0.11250005627816723, 0.11250005627816723]\n",
      "The best hyperparameter pair is [gamma,C]:\n",
      "[0.001893373197198508, 2.4924422529573533]\n",
      "with an average accuracy of:\n",
      "0.11250005627816723\n"
     ]
    }
   ],
   "source": [
    "# gamma1 = sc.reciprocal.rvs(0.001,0.1) #this works!\n",
    "# print(gamma1)\n",
    "\n",
    "# C = uniform.rvs(1,10)\n",
    "# print(C)\n",
    "\n",
    "best_pair = hyperparams[0]\n",
    "# print(len(hyperparams[0:])) #ignore the first\n",
    "print(avg_score)\n",
    "print(\"The best hyperparameter pair is [gamma,C]:\")\n",
    "print(best_pair)\n",
    "print(\"with an average accuracy of:\")\n",
    "print(avg_score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # for idx, param in enumerate\n",
    "# # kernel_SVC = Pipeline([\n",
    "# #     (\"svc\", SVC(max_iter=50000))\n",
    "# # ])\n",
    "\n",
    "# g = reciprocal(0.001,0.1)\n",
    "# print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(param_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [2 mark]\n",
    "\n",
    "**Task:** Choose the best model in (a) and (b). Then for this model, evaluate the accuracy on the test set, which is stored in `X_test` and `y_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best model in (a)\n",
    "y_preda = grid_search_cv.best_estimator_.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model in (a) has the following accuracy score on the test set:\n",
      "0.94\n"
     ]
    }
   ],
   "source": [
    "print(\"The best model in (a) has the following accuracy score on the test set:\")\n",
    "print(accuracy_score(y_preda, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best model in (b)\n",
    "\n",
    "gamma = best_pair[0] \n",
    "C = best_pair[1\n",
    "             ]\n",
    "kernel_SVC = Pipeline([\n",
    "    (\"svc\", SVC(kernel = 'rbf', max_iter=50000, gamma=gamma, C=C))\n",
    "])\n",
    "\n",
    "kernel_SVC.fit(X,y)\n",
    "y_predb = kernel_SVC.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the best model in (b) on the test set is:\n",
      "0.1135\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The accuracy score of the best model in (b) on the test set is:\")\n",
    "print(accuracy_score(y_predb, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) [3 marks]\n",
    "\n",
    "Consider the California housing data from Homework 1 using the same training and test set there. The data is obtained using the code below, which comes from Homework 1, and the training set is stored in `X` and `y`. Do not shuffle the data.\n",
    "\n",
    "Hint: Reading the solution to Question 10 in the Chapter 5 [Jupyter notebook](https://github.com/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb) on the textbook website may help with this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "\n",
    "def fetch_housing_data(housing_url, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "HOUSING_URL = (\"https://raw.githubusercontent.com/ageron/\"+\n",
    "               \"handson-ml2/master/datasets/housing/housing.tgz\")\n",
    "fetch_housing_data(HOUSING_URL)\n",
    "data = load_housing_data()\n",
    "\n",
    "data[\"income_cat\"] = np.ceil(data[\"median_income\"] / 1.5)\n",
    "data[\"income_cat\"].where(data[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(data, data[\"income_cat\"]):\n",
    "    strat_train_set = data.loc[train_index]\n",
    "    strat_test_set = data.loc[test_index]\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "X_raw = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "y = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "num_features = X_raw.drop(\"ocean_proximity\", axis=1)\n",
    "num_attribs = list(num_features)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "X = full_pipeline.fit_transform(X_raw)\n",
    "X_test_raw = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "X_test = full_pipeline.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Consider SVM regression with a Gaussian RBF kernel and a sigmoid kernel with `max_iter=50000`. For both models, use randomized search to choose good hyperparameter values for `C` and `gamma`, and set the arguement `random_state=42`. For both models, optimize the hyperparameters $C$ over the distrbution `uniform(1,10)` and $\\gamma$ over the distribution `reciprocal(0.001, 0.1)` with 10 random samples. Again, use 3-fold CV and the performance measure is MSE. What are the best hyperparameters and what is the MSE in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=SVR(max_iter=50000), n_jobs=-1,\n",
       "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000257D167C790>,\n",
       "                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000257D1809DC0>},\n",
       "                   random_state=42, scoring='neg_mean_squared_error',\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR # code based off of code in Q10 of chapter 5 jupyter notebook\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\n",
    "rnd_search_cv = RandomizedSearchCV(SVR(kernel='rbf', max_iter=50000), param_distributions, n_iter=10, verbose=2, cv=3, random_state=42, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "rnd_search_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are the following:\n",
      "SVR(C=4.745401188473625, gamma=0.07969454818643928, max_iter=50000)\n",
      "The MSE Score is:\n",
      "-13870533538.430756\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters are the following:\")\n",
    "print(rnd_search_cv.best_estimator_)\n",
    "# rnd_search_cv.cv_results_\n",
    "\n",
    "print(\"The MSE Score is:\")\n",
    "print(rnd_search_cv.best_score_)\n",
    "# print(mean_squared_error(ypred,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=SVR(kernel='sigmoid', max_iter=50000),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000257D16FD880>,\n",
       "                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000257D1806A00>},\n",
       "                   random_state=42, return_train_score=True,\n",
       "                   scoring='neg_mean_squared_error', verbose=2)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR # code based off of code in Q10 of chapter 5 jupyter notebook\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\n",
    "rnd_search_cv2 = RandomizedSearchCV(SVR(kernel='sigmoid', max_iter=50000), param_distributions, n_iter=10, verbose=2, cv=3, random_state=42, scoring=\"neg_mean_squared_error\", n_jobs=-1, return_train_score=True)\n",
    "rnd_search_cv2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are the following:\n",
      "SVR(C=4.745401188473625, gamma=0.07969454818643928, kernel='sigmoid',\n",
      "    max_iter=50000)\n",
      "The MSE Score is:\n",
      "-13740348031.432245\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters are the following:\")\n",
    "print(rnd_search_cv2.best_estimator_)\n",
    "# rnd_search_cv.cv_results_\n",
    "\n",
    "print(\"The MSE Score is:\")\n",
    "print(rnd_search_cv2.best_score_)\n",
    "# print(mean_squared_error(ypred,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) [2 marks]\n",
    "\n",
    "**Task:** Choose the best model in (d). Then for this model, evaluate the RMSE on the test set, which is stored in `X_test` and `y_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model in (d) is the SVM Regression with the sigmoid kernel\n",
      "The RMSE on the test set is:\n",
      "114752.8087714515\n"
     ]
    }
   ],
   "source": [
    "#the best model fit onto the test set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"The best model in (d) is the SVM Regression with the sigmoid kernel\")\n",
    "y_prede = rnd_search_cv2.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\"The RMSE on the test set is:\")\n",
    "print(np.sqrt(mean_squared_error(y_prede, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Voting Classifiers [8 marks]\n",
    "## (a)  [4 marks]\n",
    "\n",
    "Consider the MNIST dataset. To save computational time, split it into a smaller training set (the first 5000 observations) and a validation set (the next 1000 observations) as given by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml #code on pg 5 of lecture 4 notes\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, cache=True)\n",
    "mnist.target = mnist.target.astype(np.int8)\n",
    "\n",
    "\n",
    "N = 5000\n",
    "M = 6000\n",
    "X_train = mnist[\"data\"][:N]\n",
    "X_val  = mnist[\"data\"][N:M]\n",
    "y_train = mnist[\"target\"][:N]\n",
    "y_val = mnist[\"target\"][N:M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not shuffle the data and do not use a standard scaler. Train the following classifiers on the training set:\n",
    "\n",
    "(i) a random forest classifier with arguments `n_estimators=100, n_jobs=-1, random_state=42`,\n",
    "\n",
    "(ii) an extra-trees classifier with arguments `n_estimators=100, n_jobs=-1, random_state=42`,\n",
    "\n",
    "(iii) an AdaBoost classifier `n_estimators=50, learning_rate=0.2, random_state=42`,\n",
    "\n",
    "(iv) a gradient boosting classifier using the class `GradientBoostingClassifier()` with arguments `max_depth=2, n_estimators=10, learning_rate=0.25, random_state=42`.\n",
    "\n",
    "Report the accuracy of each trained classifier on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#(i) Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc = rfc.predict(X_val)\n",
    "rmse_rfc=np.sqrt(mean_squared_error(y_pred_rfc, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the accuracy score of the Random Forest Classifier on the Validation set:\n",
      "0.939\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is the accuracy score of the Random Forest Classifier on the Validation set:\")\n",
    "print(accuracy_score(y_val, y_pred_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(ii) Extra-trees classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "xtc = ExtraTreesClassifier(n_estimators = 100, n_jobs=-1, random_state=42)\n",
    "xtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xtc = xtc.predict(X_val)\n",
    "rmse_xtc=np.sqrt(mean_squared_error(y_pred_xtc, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the accuracy score of the Extra Trees Classifier on the Validation set:\n",
      "0.947\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is the accuracy score of the Extra Trees Classifier on the Validation set:\")\n",
    "\n",
    "print(accuracy_score(y_val, y_pred_xtc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(learning_rate=0.2, random_state=42)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(iii) AdaBoost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=50,learning_rate=0.2, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ada = ada_clf.predict(X_val)\n",
    "# rmse_ada=np.sqrt(mean_squared_error(y_pred_ada, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the accuracy score of the AdaBoost Classifier on the Validation set:\n",
      "0.736\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is the accuracy score of the AdaBoost Classifier on the Validation set:\")\n",
    "\n",
    "print(accuracy_score(y_val, y_pred_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(iv) Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier #uses decision trees as the underlying estimators (like the random forest)\n",
    "\n",
    "gbf = GradientBoostingClassifier(max_depth=2, n_estimators=10, learning_rate=0.25, random_state=42)\n",
    "gbf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_gbf = gbf.predict(X_val)\n",
    "rmse_gbf=np.sqrt(mean_squared_error(y_pred_gbf, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the accuracy score of the Gradient Boosting Classifier on the Validation set:\n",
      "0.834\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is the accuracy score of the Gradient Boosting Classifier on the Validation set:\")\n",
    "\n",
    "print(accuracy_score(y_val, y_pred_gbf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)  [4 marks]\n",
    "Train a hard-voting and a soft-voting ensemble classifier based on the models in (a). Evaluate each voting classifier on the validation set. Comment on whether the performance of the ensemble model is better or worse than the individual models in (a) and why that is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf',\n",
       "                              RandomForestClassifier(n_jobs=-1,\n",
       "                                                     random_state=42)),\n",
       "                             ('ex',\n",
       "                              ExtraTreesClassifier(n_jobs=-1, random_state=42)),\n",
       "                             ('ada',\n",
       "                              AdaBoostClassifier(learning_rate=0.2,\n",
       "                                                 random_state=42)),\n",
       "                             ('gb',\n",
       "                              GradientBoostingClassifier(learning_rate=0.25,\n",
       "                                                         max_depth=2,\n",
       "                                                         n_estimators=10,\n",
       "                                                         random_state=42))])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hard Voting Classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators=[('rf', rfc),('ex', xtc),('ada',ada_clf),('gb',gbf)], voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Hard Voting Classifier (ensemble classifier) on the validation set:\n",
      "0.923\n"
     ]
    }
   ],
   "source": [
    "y_pred_hv=voting_clf.predict(X_val)\n",
    "print(\"Accuracy of Hard Voting Classifier (ensemble classifier) on the validation set:\")\n",
    "print(accuracy_score(y_val, y_pred_hv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf',\n",
       "                              RandomForestClassifier(n_jobs=-1,\n",
       "                                                     random_state=42)),\n",
       "                             ('ex',\n",
       "                              ExtraTreesClassifier(n_jobs=-1, random_state=42)),\n",
       "                             ('ada',\n",
       "                              AdaBoostClassifier(learning_rate=0.2,\n",
       "                                                 random_state=42)),\n",
       "                             ('gb',\n",
       "                              GradientBoostingClassifier(learning_rate=0.25,\n",
       "                                                         max_depth=2,\n",
       "                                                         n_estimators=10,\n",
       "                                                         random_state=42))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Soft Voting Classifier\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('rf', rfc),('ex', xtc),('ada',ada_clf),('gb',gbf)], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Soft Voting Classifier (ensemble classifier) on the validation set:\n",
      "0.926\n"
     ]
    }
   ],
   "source": [
    "y_pred_sv=voting_clf.predict(X_val)\n",
    "print(\"Accuracy of Soft Voting Classifier (ensemble classifier) on the validation set:\")\n",
    "print(accuracy_score(y_val, y_pred_sv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest performer overall was the random forest classifier model, it outperformed all other models.\n",
      "\n",
      "Of the voting classifiers, the hard voting classifier had a higher accuracy than the soft.\n",
      "\n",
      "A reason the voting classifiers didn't outperform the underlying models is because the underlying models\n",
      "\n",
      "are not independent of one another, as they are all trained on the same day. This leads to them having\n",
      "\n",
      "the same errors as one another, leading to majority votes fort he wrong class, reducing the model's accuracy.\n"
     ]
    }
   ],
   "source": [
    "print(\"The highest performer overall was the random forest classifier model, it outperformed all other models.\")\n",
    "print(\"\\nOf the voting classifiers, the hard voting classifier had a higher accuracy than the soft.\")\n",
    "print(\"\\nA reason the voting classifiers didn't outperform the underlying models is because the underlying models\")\n",
    "print(\"\\nare not independent of one another, as they are all trained on the same day. This leads to them having\")\n",
    "print(\"\\nthe same errors as one another, leading to majority votes fort he wrong class, reducing the model's accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stacking [9 marks]\n",
    "\n",
    "We continue with the setting of Question 3. The training set, validation set and test set are the same. In Question 3, we have used predetermined rules (that is, hard-voting and soft-voting) to build the ensemble prediction. **Stacking** is an ensemble method in which you train a model (called a **blender**) to aggregate the result of each predictor into an ensemble prediction.\n",
    "\n",
    "Hint: Reading the subsection \"Stacking\" in Chapter 7 of the textbook and the solution to Question 9 in the Chapter 7 [Jupyter notebook](https://github.com/ageron/handson-ml2/blob/master/07_ensemble_learning_and_random_forests.ipynb) on the textbook website may help with this question.\n",
    "\n",
    "## (a)  [3 marks]\n",
    "\n",
    "For each of the four classifiers in Question 3(a), make 5000 clean predictions on the training set with 3-fold cross validation using `sklearn.model_selection.cross_val_predict()`. You should end up with four predictions per observation. Print at least the first 5 rows of `pred`. Next, apply one-hot encoding to `pred` since these predictions are class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml #code on pg 5 of lecture 4 notes\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, cache=True)\n",
    "mnist.target = mnist.target.astype(np.int8)\n",
    "\n",
    "\n",
    "N = 5000\n",
    "M = 6000\n",
    "X_train = mnist[\"data\"][:N]\n",
    "X_val  = mnist[\"data\"][N:M]\n",
    "y_train = mnist[\"target\"][:N]\n",
    "y_val = mnist[\"target\"][N:M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "estimators = voting_clf.estimators_ #these are the estimators from question 3\n",
    "\n",
    "X_train_predictions = np.empty((len(X_train), len(estimators)), dtype=np.float32) #on the training set, like the problem asked for\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_train_predictions[:, index] = cross_val_predict(estimator, X_train, y_train, cv=3, n_jobs=-1)\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of predictions\n",
      "[[5. 5. 3. 3.]\n",
      " [0. 0. 5. 0.]\n",
      " [4. 4. 4. 4.]\n",
      " [1. 1. 1. 1.]\n",
      " [9. 9. 9. 9.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print(\"First 5 rows of predictions\")\n",
    "print(X_train_predictions[0:5])\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "# enc.fit(X_train_predictions)\n",
    "# print(\"The one hot encoder categories\")\n",
    "# print(enc.categories_) \n",
    "\n",
    "X_tp = enc.fit_transform(X_train_predictions) #transforms the predictions into one hot encoding predictions\n",
    "\n",
    "X_tp = X_tp.toarray()\n",
    "print(X_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "X_train_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [3 marks]\n",
    "Use the predictions in (a) as features and the actual label of the observations as the target. Train a random forest classifier on the training set with the parameters `n_estimators=100, random_state=42`.  This classifier is a blender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc.fit(X_tp, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [3 marks]\n",
    "\n",
    "Obtain the predictions of the blender on the validation set by feeding predictions on the validation set from the four classifiers in Question 3(a) into the blender trained in Question 4(b). Do not retrain the blender. These are called stacking predictions. Report the accuracy of your stacking predictions on the validation set and compare this to the results in Question 3(b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea is from Q9 of ch 7 jupyter notebook\n",
    "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32) #on the training set, like the problem asked for\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_val_predictions[:, index] = estimator.predict(X_val)\n",
    "\n",
    "enc_c = OneHotEncoder()\n",
    "enc_c.fit(X_val_predictions)\n",
    "    \n",
    "X_tval = enc_c.transform(X_val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the stacking predictions on the validation set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.947"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predy4c = rfc.predict(X_tval)\n",
    "\n",
    "print(\"The accuracy of the stacking predictions on the validation set.\")\n",
    "accuracy_score(predy4c, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
